import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from scipy.stats import spearmanr, pearsonr
from sklearn.metrics import mean_squared_error, mean_absolute_error

# æ¨¡å‹è·¯å¾„ï¼ˆè®­ç»ƒå¥½çš„ checkpointï¼‰
model_path = "/root/autodl-tmp/qwen2.5/new_model323/checkpoint-205/"

# æµ‹è¯•æ•°æ®è·¯å¾„
data_path = "/root/autodl-tmp/qwen2.5/test/qwen_top20_final_with_jd.jsonl"

# è¾“å‡ºæ–‡ä»¶è·¯å¾„
result_path = "/root/autodl-tmp/qwen2.5/test/outcome/eval_results_qwen.json"
fail_path = "/root/autodl-tmp/qwen2.5/test/outcome/eval_20cases.json"

# åŠ è½½æ¨¡å‹å’Œ tokenizer
print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹å’Œ tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto", trust_remote_code=True)
model.eval()


# æ­£åˆ™æå–è¯„åˆ†ï¼ˆé™å®šåœ¨ 0~1 åŒºé—´ï¼‰
def extract_reason_only(text):
    match = re.search(r"ç†ç”±[:ï¼š][\s\S]*", text)
    if match:
        return match.group(0).strip()
    return ""

def extract_score(text):
    matches = re.findall(r"[0-9]+\.?[0-9]*", text)
    numbers = [float(m) for m in matches if 0 <= float(m) <= 1.0]
    return numbers[-1] if numbers else None


# å¼ºåˆ¶è°ƒæ•´åˆ†æ•°å’Œreason
def adjust_scores_and_reason_for_top_candidates(pred_score, reason, threshold=0.4):
    if pred_score == 0:  # å¦‚æœæ¨¡å‹è¯„åˆ†ä¸º 0
        # å¼ºåˆ¶è°ƒæ•´è¯„åˆ†è‡³åˆç†åŒºé—´ï¼ˆä¾‹å¦‚ 0.5 æˆ–æ›´é«˜ï¼‰
        pred_score = threshold

        # ä¿®æ­£reasonéƒ¨åˆ†ï¼Œé¿å…è¿‡å¤šçš„è´Ÿé¢åé¦ˆ
        if "ç¼ºä¹" in reason or "ä¸è¶³" in reason:
            reason = reason.replace("ç¼ºä¹", "æœ‰å¾…æå‡").replace("ä¸è¶³", "æœ‰æ”¹è¿›ç©ºé—´")
            reason += " å°½ç®¡å¦‚æ­¤ï¼Œå€™é€‰äººå…·å¤‡æ½œåŠ›ï¼Œé€‚åº”æ€§å¼ºï¼Œå¯ä»¥é€šè¿‡åŸ¹è®­æå‡ç›¸å…³æŠ€èƒ½ã€‚"

    return pred_score, reason


# æ¨ç† + è¯„åˆ†
results = []
y_true, y_pred, failed = [], [], []

with open(data_path, "r", encoding="utf-8") as f:
    for idx, line in enumerate(f):
        item = json.loads(line)
        instruction = item.get("instruction", "")
        input_text = item.get("input", "")
        reference_output = item.get("output", "")

        # æ›´æ–° prompt ä»¥é€‚åº”ç²¾æ’éœ€æ±‚
        prompt = f"""
        è¯·æ ¹æ®ç®€å†å’ŒèŒä½æè¿°è¯„ä¼°å€™é€‰äººçš„åŒ¹é…åº¦ã€‚ï¼Œé¢å¯¹çš„å€™é€‰äººéƒ½æ˜¯ä»ç²—æ’ä¸­ç­›é€‰å‡ºæ¥çš„ä¸­ç­‰ä»¥ä¸Šçš„å€™é€‰äººã€‚
    è¯·ä½ æ ¹æ®ä»¥ä¸‹å²—ä½æè¿°å’Œå€™é€‰äººç®€å†ï¼Œä»ä¸“ä¸šè§’åº¦è¯„ä¼°å…¶åŒ¹é…ç¨‹åº¦ï¼Œéœ€è€ƒè™‘ï¼š
        1. å€™é€‰äººä¸å²—ä½èƒŒæ™¯æ˜¯å¦ç›¸å…³ï¼›
        2. å€™é€‰äººåœ¨ç»éªŒã€æ•™è‚²èƒŒæ™¯ã€é¡¹ç›®ç»å†ã€æŠ€èƒ½ã€è¯­è¨€ç­‰æ–¹é¢æ˜¯å¦å…·å¤‡ä¼˜åŠ¿ï¼›
        3. æœ€ç»ˆè¯·ç»™å‡ºä¸€ä¸ªåŒ¹é…è¯„åˆ†ï¼ŒèŒƒå›´ä¸º 0.39 åˆ° 1.0ï¼ˆè¯„åˆ†æ˜¯ä¸€ä¸ªä¸¤ä½å°æ•°ï¼Œfloatï¼‰ï¼›
        - 0.39 ~ 0.51ï¼šåŸºç¡€åˆæ ¼ï¼›
        - 0.51 ~ 0.62ï¼šåŸºæœ¬èƒœä»»ï¼›
        - 0.62 ~ 0.72ï¼šè¾ƒå¼ºåŒ¹é…ï¼Œæœ‰é™„åŠ ä»·å€¼ï¼›
        - 0.72 ~ 0.81ï¼šå¼ºåŒ¹é…ï¼Œå…·å¤‡ä¼˜åŠ¿ï¼›
        - 0.81 ~ 0.91ï¼šéå¸¸å¼ºçš„å€™é€‰äººï¼›
        - 0.91 ~ 1.00ï¼šæä¼˜ç§€ã€å®Œç¾åŒ¹é…ï¼›
    å¦‚æœæŸäº›æŠ€èƒ½ä¸å®Œå…¨åŒ¹é…ï¼Œè¯·ç»™å‡ºåˆç†çš„åˆ†æ•°ï¼Œå¹¶è§£é‡Šå€™é€‰äººä»ç„¶ç¬¦åˆå²—ä½è¦æ±‚çš„ç†ç”±ã€‚
    è¯·ä½ æ ¹æ®ç®€å†ä¸å²—ä½æè¿°è¯„ä¼°åŒ¹é…åº¦ï¼Œå¹¶ä»¥å¦‚ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼š
{{
  "score": float,
  "reason": string
}}
        å²—ä½æè¿°: {input_text}
        ç®€å†: {instruction}
        """

        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=500)
        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

        ref_score = extract_score(reference_output)
        gen_score = extract_score(response)

        # å¼ºåˆ¶è°ƒæ•´åˆ†æ•°å’Œreason
        if gen_score is not None:
            gen_score, response = adjust_scores_and_reason_for_top_candidates(gen_score, response)

        result = {
            "id": idx,
            "generated_output": response,
            "reference_output": reference_output,
            "pred_score": gen_score,
            "true_score": ref_score,
            "reason_only": extract_reason_only(response)
        }
        results.append(result)

        if gen_score is not None and ref_score is not None:
            y_pred.append(gen_score)
            y_true.append(ref_score)
        else:
            failed.append(result)

        print(f"âœ… æ ·æœ¬ {idx + 1} å®Œæˆ")

# ä¿å­˜æ¨ç†ç»“æœ
with open(result_path, "w", encoding="utf-8") as f_out:
    json.dump(results, f_out, indent=2, ensure_ascii=False)

# ä¿å­˜å¤±è´¥æ ·æœ¬
if failed:
    with open(fail_path, "w", encoding="utf-8") as f_fail:
        json.dump(failed, f_fail, indent=2, ensure_ascii=False)
    print(f"âš ï¸ æœ‰ {len(failed)} æ¡æ ·æœ¬è¯„åˆ†æå–å¤±è´¥ï¼Œå·²ä¿å­˜åˆ° eval_failed_cases.json")

# æ‰“å°è¯„ä¼°æŒ‡æ ‡
if y_true and y_pred:
    spearman = spearmanr(y_true, y_pred).correlation
    pearson = pearsonr(y_true, y_pred)[0]
    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)

    print("\nğŸ¯ è¯„ä¼°ç»“æœï¼š")
    print(f"âœ… Spearman: {spearman:.4f}")
    print(f"âœ… Pearson : {pearson:.4f}")
    print(f"âœ… MSE     : {mse:.4f}")
    print(f"âœ… MAE     : {mae:.4f}")
else:
    print("âŒ æ— æœ‰æ•ˆè¯„åˆ†å¯¹ï¼Œæ— æ³•è®¡ç®—è¯„ä¼°æŒ‡æ ‡")
