import os
import pandas as pd
from paddleocr import PaddleOCR
from pdf2image import convert_from_path
import Levenshtein
import numpy as np
import re

# ==== åˆå§‹åŒ– OCR ====
ocr = PaddleOCR(use_angle_cls=True, lang='en')  # è‹±æ–‡æ¨¡å‹

# ==== è·¯å¾„é…ç½® ====
csv_path = "/root/autodl-tmp/paddle328/data/csv/_20_____.csv"
pdf_dir = "/root/autodl-tmp/paddle328/data/image-en/"
output_dir = "/root/autodl-tmp/paddle328/outcome/paddle/"
os.makedirs(output_dir, exist_ok=True)
output_file = os.path.join(output_dir, "ocr_eval_summary.txt")

# ==== åŠ è½½çœŸå®æ–‡æœ¬ ====
df = pd.read_csv(csv_path)
id_to_text = dict(zip(df['ID'].astype(str), df['Resume_str'].astype(str)))

# ==== æŒ‡æ ‡ç´¯è®¡å˜é‡ ====
total_chars = total_words = total_char_errs = total_word_errs = 0
total_confidence = 0.0
matched_files = 0

# ==== éå† PDF ====
for filename in os.listdir(pdf_dir):
    if not filename.endswith(".pdf"):
        continue

    file_id_match = re.findall(r"\d+", filename)
    if not file_id_match:
        continue
    file_id = file_id_match[0]

    if file_id not in id_to_text:
        continue

    true_text = id_to_text[file_id]
    true_words = true_text.split()
    total_chars += len(true_text)
    total_words += len(true_words)

    pdf_path = os.path.join(pdf_dir, filename)
    pages = convert_from_path(pdf_path, dpi=300)

    ocr_text = ""
    confidences = []

    for page_num, img in enumerate(pages):
        try:
            img_np = np.array(img)
            result = ocr.ocr(img_np, cls=True)

            if result and result[0]:
                for line in result[0]:
                    text = line[1][0].strip()
                    conf = line[1][1]
                    ocr_text += text + " "
                    confidences.append(conf)
        except Exception as e:
            print(f"[âš ï¸ ERROR] æ–‡ä»¶ {filename} ç¬¬ {page_num+1} é¡µè¯†åˆ«å¤±è´¥ï¼Œè·³è¿‡ã€‚é”™è¯¯ï¼š{e}")

    ocr_text = ocr_text.strip()
    ocr_words = ocr_text.split()

    char_distance = Levenshtein.distance(ocr_text, true_text)
    word_distance = Levenshtein.distance(" ".join(ocr_words), " ".join(true_words))
    cer = char_distance / max(1, len(true_text))
    wer = word_distance / max(1, len(true_words))
    avg_conf = np.mean(confidences) if confidences else 0.0

    total_char_errs += char_distance
    total_word_errs += word_distance
    total_confidence += avg_conf
    matched_files += 1

    # å•æ–‡ä»¶è¾“å‡º
    print(f"\nğŸ“„ æ–‡ä»¶ï¼š{filename}")
    print(f"å­—ç¬¦çº§å‡†ç¡®ç‡ï¼š{1 - cer:.2%}")
    print(f"å•è¯çº§å‡†ç¡®ç‡ï¼š{1 - wer:.2%}")
    print(f"CERï¼ˆå­—ç¬¦é”™è¯¯ç‡ï¼‰ï¼š{cer:.2%}")
    print(f"WERï¼ˆå•è¯é”™è¯¯ç‡ï¼‰ï¼š{wer:.2%}")
    print(f"OCR å¹³å‡ç½®ä¿¡åº¦ï¼š{avg_conf:.2%}")

# ==== æ±‡æ€»è¾“å‡º ====
avg_char_acc = 1 - total_char_errs / max(1, total_chars)
avg_word_acc = 1 - total_word_errs / max(1, total_words)
avg_cer = total_char_errs / max(1, total_chars)
avg_wer = total_word_errs / max(1, total_words)
avg_conf = total_confidence / max(1, matched_files)

summary = f"""
====== ğŸ“Š æ€»ä½“è¯„ä¼° ======
å¹³å‡å­—ç¬¦å‡†ç¡®ç‡ï¼š{avg_char_acc:.2%}
å¹³å‡å•è¯å‡†ç¡®ç‡ï¼š{avg_word_acc:.2%}
CERï¼ˆå¹³å‡å­—ç¬¦é”™è¯¯ç‡ï¼‰ï¼š{avg_cer:.2%}
WERï¼ˆå¹³å‡å•è¯é”™è¯¯ç‡ï¼‰ï¼š{avg_wer:.2%}
OCR å¹³å‡ç½®ä¿¡åº¦ï¼š{avg_conf:.2%}
å…±å¤„ç†æ–‡æ¡£ï¼š{matched_files}
"""

print(summary)

# ==== ä¿å­˜æ–‡ä»¶ ====
with open(output_file, "w", encoding="utf-8") as f:
    f.write(summary)
